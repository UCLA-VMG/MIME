{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pet Images Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5Pb44HhmI2x",
    "outputId": "57c188e1-9fea-4f7b-ca6f-d3e17f9ec99b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.image as img\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "sd = 0\n",
    "np.random.seed(sd)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(sd)\n",
    "random.seed(sd)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CqLxOSQmMk9",
    "outputId": "60dc2cd1-f11f-487a-d788-fe88b36e24f4"
   },
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "def equalize_all(dct):\n",
    "    lens = []\n",
    "    for k in dct.keys():\n",
    "        lens.append(len(dct[k]))\n",
    "    \n",
    "    l_min = min(lens)\n",
    "    dct_new = {}\n",
    "    for k in dct.keys():\n",
    "        list_temp = dct[k]\n",
    "        dct_new[k] = list_temp[0:l_min]\n",
    "    return dct_new\n",
    "\n",
    "def test_lengths(dct):\n",
    "    lens = []\n",
    "    for k in dct.keys():\n",
    "        lens.append(len(dct[k]))\n",
    "    \n",
    "    return lens\n",
    "\n",
    "\n",
    "dict_animal_to_number = {'cats' : 0,'dogs': 1}\n",
    "dict_color_to_number = {'dark' : 0,'light': 1}\n",
    "\n",
    "data_path = r'./data'\n",
    "animal_list = ['cats','dogs']\n",
    "\n",
    "col_list = ['dark','light']\n",
    "\n",
    "sets = ['train','test']\n",
    "\n",
    "train_data_dict = {}\n",
    "val_data_dict = {}\n",
    "test_data_dict = {}\n",
    "\n",
    "for col in col_list:\n",
    "    for animal in animal_list:\n",
    "        f_path = data_path+'/'+animal+'/'+col+'/'\n",
    "        all_f = os.listdir(f_path)\n",
    "        all_f = [f_path + s for s in all_f]\n",
    "        random.shuffle(all_f)\n",
    "        l_temp = len(all_f)\n",
    "        \n",
    "        train_data_dict[col+animal] = all_f[0:int(0.75*l_temp)]\n",
    "        val_data_dict[col+animal] = all_f[int(0.75*l_temp):int(0.875*l_temp)]\n",
    "        test_data_dict[col+animal] = all_f[int(0.875*l_temp):]\n",
    "train_data_dict = equalize_all(train_data_dict)\n",
    "\n",
    "        \n",
    "val_data_dict = equalize_all(val_data_dict)\n",
    "test_data_dict = equalize_all(test_data_dict)\n",
    "l1 = test_lengths(train_data_dict)\n",
    "print(l1)\n",
    "l1 = test_lengths(val_data_dict)\n",
    "print(l1)\n",
    "l1 = test_lengths(test_data_dict)\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TuRR4A3meJV"
   },
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, data, path , transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "#         img_name,label = self.data[index]\n",
    "        img_name = self.data[index][0]\n",
    "        label = dict_animal_to_number[self.data[index][2]]   # index 2 for shape, need as tensor -> convert to number from str first\n",
    "        label = torch.tensor(label)\n",
    "        img_path = img_name#os.path.join(self.path, img_name)\n",
    "        image0 = cv2.imread(img_path)\n",
    "        [h, w, ch] = image0.shape\n",
    "        mini = min(h,w)\n",
    "        if mini%2==1:\n",
    "          mini = mini-1\n",
    "        image1 = image0[int((h-mini)/2):int((h+mini)/2),int((w-mini)/2):int((w+mini)/2),:]\n",
    "        image = cv2.resize(image1, (256,256))# resize(image0, (28, 28))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kySdmHomonC"
   },
   "outputs": [],
   "source": [
    "#Transforms go here\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "valid_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdFfvIwImuIz"
   },
   "outputs": [],
   "source": [
    "#Functions to resample and equalize datasets\n",
    "def resample_dataset_color(data,frac,protected_var,task_var):\n",
    "    #frac wrt first group class\n",
    "    frac_list = [frac,1-frac]\n",
    "    protected_trait = []\n",
    "    task_trait = []\n",
    "    file_name = []\n",
    "    ix = 0\n",
    "    for pr in protected_var:\n",
    "        for ta in task_var:\n",
    "            \n",
    "            list_temp = data[pr+ta]\n",
    "            list_temp = list_temp[0:int(frac_list[ix]*len(list_temp))]\n",
    "            \n",
    "            for l in list_temp:\n",
    "                file_name.append(l)\n",
    "                protected_trait.append(pr)\n",
    "                task_trait.append(ta)\n",
    "           \n",
    "        ix = ix+1\n",
    "    \n",
    "    df = {'file': file_name, 'color': protected_trait, 'animal': task_trait}\n",
    "\n",
    "    dat_split = df = pd.DataFrame(data=df)\n",
    "    \n",
    "    dat_split = dat_split.sample(frac=1)\n",
    "    \n",
    "    \n",
    "    return dat_split\n",
    "\n",
    "def split_two(data,protected_var,task_var):\n",
    "    \n",
    "    ix = 0\n",
    "    for pr in protected_var:\n",
    "        protected_trait = []\n",
    "        task_trait = []\n",
    "        file_name = []\n",
    "        for ta in task_var:\n",
    "            \n",
    "            list_temp = data[pr+ta]\n",
    "            \n",
    "            \n",
    "            for l in list_temp:\n",
    "                file_name.append(l)\n",
    "                protected_trait.append(pr)\n",
    "                task_trait.append(ta)\n",
    "           \n",
    "        \n",
    "    \n",
    "        df = {'file': file_name, 'color': protected_trait, 'animal': task_trait}\n",
    "        \n",
    "        if ix==0:\n",
    "            dat_split0 = df = pd.DataFrame(data=df)\n",
    "            dat_split0 = dat_split0.sample(frac=1)\n",
    "        else:\n",
    "            dat_split1 = df = pd.DataFrame(data=df)\n",
    "            dat_split1 = dat_split1.sample(frac=1)\n",
    "\n",
    "        ix = ix+1\n",
    "    return dat_split0,dat_split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P77IUfQRmuwL",
    "outputId": "448ec6e6-5308-491a-b77f-44688d119758"
   },
   "outputs": [],
   "source": [
    "newD2 = resample_dataset_color(train_data_dict,0.5,col_list,animal_list)\n",
    "print(newD2['color'].value_counts())\n",
    "print(newD2)\n",
    "batch_size = 30\n",
    "\n",
    "val_split_D,val_split_L = split_two(val_data_dict,col_list,animal_list)\n",
    "val_split = val_split_L\n",
    "\n",
    "# print(val_split)\n",
    "print(val_split['color'].value_counts())\n",
    "print(val_split['animal'].value_counts())\n",
    "\n",
    "# ##split into class wise test samples\n",
    "test_split_D,test_split_L = split_two(test_data_dict,col_list,animal_list)\n",
    "print(test_split_D['color'].value_counts())\n",
    "print(test_split_D['animal'].value_counts())\n",
    "print(test_split_L['color'].value_counts())\n",
    "print(test_split_L['animal'].value_counts())\n",
    "\n",
    "#dataloaders\n",
    "valid_data = PetDataset(val_split, data_path, valid_transform )\n",
    "test_data_D = PetDataset(test_split_D, data_path, test_transform )\n",
    "test_data_L = PetDataset(test_split_L, data_path, test_transform )\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "test_loader_D = DataLoader(dataset = test_data_D, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "test_loader_L = DataLoader(dataset = test_data_L, batch_size = batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryiBGfbfmqns",
    "outputId": "84ecd6b9-0c14-48fb-e4e8-46d68ed68010"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R98nkeNjmz3E"
   },
   "outputs": [],
   "source": [
    "def test_performance(model,dataL,criterion):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    temp_test_acc = []\n",
    "\n",
    "    for data, target in dataL:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        # update-average-validation-loss \n",
    "        test_loss += loss.item() * data.size(0)\n",
    "\n",
    "        op_temp = output.detach().cpu().numpy()\n",
    "        op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "        test_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "\n",
    "    ttacc  = test_acc/len(dataL.sampler)\n",
    "    test_loss_M = test_loss/len(dataL.sampler)\n",
    "    \n",
    "    test_print = 'Test Loss: {:.3f} \\tTest Acc: {:.3f}'.format(\n",
    "        test_loss_M, ttacc)\n",
    "\n",
    "    print(test_print)\n",
    "    return test_print, ttacc\n",
    "\n",
    "def write_file(fname,string,act):\n",
    "    with open(fname, act) as text_file:\n",
    "        text_file.write(string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYC-73OIm2pZ",
    "outputId": "119d2ce4-685f-44c3-a6b3-9b42621f5fab"
   },
   "outputs": [],
   "source": [
    "#Combined iterations across minority training ratios\n",
    "def train_model(dark_frac,train_data_dict,valid_loader,test_loader_D,test_loader_L):\n",
    "  num_epochs = 60\n",
    "  num_classes = 2  # for animals\n",
    "  batch_size = 30\n",
    "  learning_rate = 0.0006\n",
    "\n",
    "  check_point_dir = col_list[0]+str(dark_frac)\n",
    "  \n",
    "  if not os.path.isdir(f\"checkpoints/\"+check_point_dir):\n",
    "      os.makedirs(f\"checkpoints/\"+check_point_dir)\n",
    "      print(\"Output directory is created\")\n",
    "      \n",
    "  #make logger text file\n",
    "  text_path = f\"checkpoints/\"+check_point_dir+\"/\"+\"log.txt\"\n",
    "  try:\n",
    "      os.remove(text_path)\n",
    "  except OSError:\n",
    "      pass\n",
    "  \n",
    "  write_file(text_path,'*********'+col_list[0]+'  fraction: {} *********'.format(dark_frac),'a')\n",
    "  \n",
    "  train_split = resample_dataset_color(train_data_dict,dark_frac,col_list,animal_list)\n",
    "  print(train_split['color'].value_counts())\n",
    "  print(train_split['animal'].value_counts())\n",
    "  write_file(text_path,str(train_split['color'].value_counts()),'a')\n",
    "  \n",
    "  write_file(text_path,str(train_split['animal'].value_counts()),'a')\n",
    "  \n",
    "  #Dataloaders\n",
    "  train_data = PetDataset(train_split, data_path, train_transform )\n",
    "\n",
    "  train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "  \n",
    "  model = models.resnet34(pretrained=False)\n",
    "  model.fc = nn.Linear(512, num_classes)\n",
    "  model.load_state_dict(torch.load(f\"resnet34_imp_2class.pt\"))\n",
    "  model.to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  optimizer = torch.optim.AdamW(\n",
    "      model.parameters(), \n",
    "      lr=learning_rate, \n",
    "      betas=(0.5, 0.999), \n",
    "      weight_decay=0.05\n",
    "      )\n",
    "\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "      optimizer, T_max=30, \n",
    "      eta_min=0.01 * learning_rate, verbose=True\n",
    "      )\n",
    "  \n",
    "  \n",
    "  \n",
    "  # Actual training of model\n",
    "\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "\n",
    "  train_accuracies = []\n",
    "  val_accuracies = []\n",
    "\n",
    "  print(\"Training model...\")\n",
    "  valid_accuracy = []\n",
    "  test_accuracy_light = []\n",
    "  test_accuracy_dark = []\n",
    "\n",
    "  best_val_acc = 0\n",
    "\n",
    "  for epoch in range(1, num_epochs+1):\n",
    "      # keep track of train/val loss\n",
    "      train_loss = 0.0\n",
    "      valid_loss = 0.0\n",
    "\n",
    "      # training the model\n",
    "      model.train()\n",
    "      temp_train_acc = 0.0\n",
    "      for data, target in train_loader:\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "\n",
    "          optimizer.zero_grad()                   # init gradients to zeros\n",
    "          output = model(data)                    # forward pass\n",
    "      #         print(output)\n",
    "      #         print(target)\n",
    "          loss = criterion(output, target)        # compute loss\n",
    "          loss.backward()                         # loss backwards\n",
    "          optimizer.step()                        # update model params\n",
    "\n",
    "          train_loss += loss.item() * data.size(0)\n",
    "\n",
    "          op_temp = output.detach().cpu().numpy()\n",
    "          op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "          temp_train_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "          \n",
    "      \n",
    "      # validate-the-model\n",
    "      model.eval()\n",
    "      temp_val_acc = 0.0\n",
    "      for data, target in valid_loader:\n",
    "\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "\n",
    "          output = model(data)\n",
    "\n",
    "          loss = criterion(output, target)\n",
    "\n",
    "          # update-average-validation-loss \n",
    "          valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "          op_temp = output.detach().cpu().numpy()\n",
    "          op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "          temp_val_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "\n",
    "      tvacc  = np.mean(np.array(temp_val_acc))\n",
    "\n",
    "      if tvacc>best_val_acc:\n",
    "          best_val_acc = tvacc\n",
    "          torch.save(model.state_dict(), f\"checkpoints/\"+check_point_dir+\"/model_best.pt\")\n",
    "          print('Model saved')\n",
    "          write_file(text_path,'Model saved','a')\n",
    "\n",
    "      # calculate-average-losses\n",
    "      train_loss = train_loss/len(train_loader.sampler)\n",
    "      valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "      \n",
    "      ttacc  = temp_train_acc/len(train_loader.sampler)\n",
    "      tvacc  = temp_val_acc/len(valid_loader.sampler)\n",
    "      \n",
    "      train_losses.append(train_loss)\n",
    "      valid_losses.append(valid_loss)\n",
    "\n",
    "      train_accuracies.append(ttacc)\n",
    "      val_accuracies.append(tvacc)\n",
    "\n",
    "      scheduler.step()\n",
    "\n",
    "      # print-training/validation-statistics \n",
    "      train_print = 'Epoch: {} \\tTr Loss: {:.3f} \\tTr Acc: {:.3f} \\tVal Loss: {:.3f} \\tVal Acc: {:.3f}'.format(\n",
    "          epoch, train_loss, ttacc, valid_loss, tvacc)\n",
    "      print(train_print)\n",
    "\n",
    "      test_print_D, ttacc_D = test_performance(model,test_loader_D,criterion)\n",
    "      test_print_L, ttacc_L = test_performance(model,test_loader_L,criterion)\n",
    "      valid_accuracy.append(tvacc)\n",
    "      test_accuracy_dark.append(ttacc_D)\n",
    "      test_accuracy_light.append(ttacc_L)\n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      write_file(text_path,train_print,'a')\n",
    "\n",
    "      write_file(text_path,test_print_D,'a')\n",
    "      \n",
    "      write_file(text_path,test_print_L,'a')\n",
    "  path_val = f\"checkpoints/\"+check_point_dir+\"/\"+\"validation_accuracy\"\n",
    "  path_dark = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_D\"\n",
    "  path_light = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_L\"\n",
    "  valid_accuracy = np.array(valid_accuracy)\n",
    "  test_accuracy_dark = np.array(test_accuracy_dark)\n",
    "  test_accuracy_light = np.array(test_accuracy_light)\n",
    "  np.save(path_val, valid_accuracy)\n",
    "  np.save(path_dark, test_accuracy_dark)\n",
    "  np.save(path_light, test_accuracy_light)\n",
    "\n",
    "\n",
    "dark_fracs = np.linspace(0.0,1.0,11)\n",
    "for dark_frac in dark_fracs:\n",
    "    print('********* dark fraction: {} *********'.format(dark_frac))\n",
    "    train_model(dark_frac,train_data_dict,valid_loader,test_loader_D,test_loader_L)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cats_2.0_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
