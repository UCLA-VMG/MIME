{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14607,
     "status": "ok",
     "timestamp": 1636275664032,
     "user": {
      "displayName": "Pradyumna Chari",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18236218636442071478"
     },
     "user_tz": 480
    },
    "id": "O4S8QwoDBm0A",
    "outputId": "1bbecf5f-1e86-43dd-fead-8eed6c7062e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.image as img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VXA_TAJARJQ"
   },
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "sd = 0\n",
    "np.random.seed(sd)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(sd)\n",
    "random.seed(sd)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scx03GScUYqX"
   },
   "outputs": [],
   "source": [
    "header = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "                  'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
    "\n",
    "df_temp = pd.read_csv(\"data/adults.csv\",index_col=False, skipinitialspace=True,header=None,names=header)\n",
    "df = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1636275665248,
     "user": {
      "displayName": "Pradyumna Chari",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18236218636442071478"
     },
     "user_tz": 480
    },
    "id": "V-RsevouVFeM",
    "outputId": "f4f6ccd5-43b8-4363-9aa8-6072d8427083"
   },
   "outputs": [],
   "source": [
    "## Drop rows with NaN value\n",
    "df = df.replace('?', np.nan)\n",
    "df[pd.isnull(df).any(axis=1)].shape\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVZfvhPhl0sd"
   },
   "outputs": [],
   "source": [
    "##drop the education-num column because there is education column\n",
    "df.drop('education-num', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jEuZ4iOlxK4"
   },
   "outputs": [],
   "source": [
    "#Convert categoricals to numerical\n",
    "\n",
    "def convert_to_int(columns):\n",
    "  for column in columns:\n",
    "    unique_values = df[column].unique().tolist()\n",
    "    dic = {}\n",
    "    for indx, val in enumerate(unique_values):\n",
    "      dic[val]=indx\n",
    "    df[column] = df[column].map(dic).astype(int)\n",
    "    print(column + \" done!\")\n",
    "\n",
    "def convert_to_onehot(data,columns):\n",
    "  dummies = pd.get_dummies(data[columns])\n",
    "  data = data.drop(columns, axis=1)\n",
    "  data = pd.concat([data, dummies], axis=1)\n",
    "  return data\n",
    "\n",
    "categorical_columns = ['workclass','education','marital-status','occupation','relationship','sex','native-country']#,'race'\n",
    "label_column = ['salary']\n",
    "\n",
    "convert_to_int(label_column)\n",
    "show_unique_values(label_column)\n",
    "\n",
    "df = convert_to_onehot(df,categorical_columns)\n",
    "df = convert_to_onehot(df,['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aH3VAijVPRWg"
   },
   "outputs": [],
   "source": [
    "#Normalize columns\n",
    "normalize_columns = ['age', 'fnlwgt', 'capital-gain','capital-loss','hours-per-week']\n",
    "\n",
    "def normalize(columns):\n",
    "  scaler = preprocessing.StandardScaler()\n",
    "  df[columns] = scaler.fit_transform(df[columns])\n",
    "normalize(normalize_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt7y_-09EMQ6"
   },
   "outputs": [],
   "source": [
    "class AdultsDataset(Dataset):\n",
    "    def __init__(self, data,labels):\n",
    "        super().__init__()\n",
    "        self.data = data.values\n",
    "        self.label = labels.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        dat = torch.tensor(self.data[index])\n",
    "        label = torch.tensor(self.label[index]) \n",
    "\n",
    "        return dat, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0L9DJeqHWbH"
   },
   "outputs": [],
   "source": [
    "#functions for resampling data\n",
    "def resample_dataset_race(data,frac):\n",
    "    flagsD = data['sex_Male']==1#'Black'\n",
    "    flagsL = data['sex_Female']==1#'White'\n",
    "    data_D = data[flagsD]\n",
    "    data_L = data[flagsL]\n",
    "    \n",
    "    data_D = equalize_dataset_gender(data_D,0.5)\n",
    "    data_L = equalize_dataset_gender(data_L,0.5)\n",
    "    \n",
    "    baseline = min(len(data_D),len(data_L))-2\n",
    "    \n",
    "    data_D = data_D.sort_values('salary')\n",
    "    data_L = data_L.sort_values('salary')\n",
    "    \n",
    "    remD = len(data_D)-int((frac)*baseline)\n",
    "    tempD = data_D[int(0.5*remD):-int(0.5*remD)]\n",
    "    remL = len(data_L)-int((1-frac)*baseline)\n",
    "    tempL = data_L[int(0.5*remL):-int(0.5*remL)]\n",
    "    \n",
    "    print(baseline,remD,remL)\n",
    "    \n",
    "    \n",
    "    \n",
    "    frames = [tempD,tempL]\n",
    "    final_split = pd.concat(frames)\n",
    "    final_split = final_split.sample(frac=1)\n",
    "    \n",
    "    return final_split\n",
    "\n",
    "def resample_dataset_race_old(data,frac):\n",
    "    flagsD = data['sex_Male']==1#'Black'\n",
    "    flagsL = data['sex_Female']==1#'White'\n",
    "    data_D = data[flagsD]\n",
    "    data_L = data[flagsL]\n",
    "    \n",
    "    \n",
    "    baseline = min(len(data_D),len(data_L))\n",
    "    \n",
    "    \n",
    "    tempD = data_D[0:int(frac*baseline)]\n",
    "    tempL = data_L[0:int((1-frac)*baseline)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    frames = [tempD,tempL]\n",
    "    final_split = pd.concat(frames)\n",
    "    final_split = final_split.sample(frac=1)\n",
    "    \n",
    "    return final_split\n",
    "\n",
    "def equalize_dataset_gender(data,frac):\n",
    "    flagsM = data['salary']==0\n",
    "    flagsF = data['salary']==1\n",
    "    data_M = data[flagsM]\n",
    "    data_F = data[flagsF]\n",
    "    baseline = min(len(data_M),len(data_F))\n",
    "    \n",
    "    tempM = data_M[0:int(baseline)]\n",
    "    tempF = data_F[0:int(baseline)]\n",
    "    frames = [tempM,tempF]\n",
    "    final_split = pd.concat(frames)\n",
    "    final_split = final_split.sample(frac=1)\n",
    "    \n",
    "    return final_split\n",
    "\n",
    "\n",
    "#functions for data\n",
    "def resample_dataset_equal(data):\n",
    "    flagsD = data['sex_Male']==1#'Black'\n",
    "    flagsL = data['sex_Female']==1#'White'\n",
    "    data_D = data[flagsD]\n",
    "    data_L = data[flagsL]\n",
    "    \n",
    "    data_D = equalize_dataset_gender(data_D,0.5)\n",
    "    data_L = equalize_dataset_gender(data_L,0.5)\n",
    "    \n",
    "    baseline = min(len(data_D),len(data_L))\n",
    "    \n",
    "    data_D = data_D.sort_values('salary')\n",
    "    data_L = data_L.sort_values('salary')\n",
    "    \n",
    "    remD = len(data_D)-int((0.5)*baseline)\n",
    "    tempD = data_D[int(0.5*remD):-int(0.5*remD)]\n",
    "    remL = len(data_L)-int((0.5)*baseline)\n",
    "    tempL = data_L[int(0.5*remL):-int(0.5*remL)]\n",
    "\n",
    "    \n",
    "    return tempD,tempL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXVmACSDNpiA"
   },
   "outputs": [],
   "source": [
    "train_labels = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1636275666154,
     "user": {
      "displayName": "Pradyumna Chari",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18236218636442071478"
     },
     "user_tz": 480
    },
    "id": "gw6zXBp3HWbI",
    "outputId": "3108df5f-809e-439d-bdc1-d94a66700799"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 30\n",
    "# learning_rate = 0.0006\n",
    "    \n",
    "#generate base data\n",
    "train_split_t, val_split_t_dash = train_test_split(train_labels, test_size=0.2)\n",
    "# print(val_split_t)\n",
    "val_split_t, test_labels = train_test_split(val_split_t_dash, test_size=0.96)\n",
    "\n",
    "#Ensure val set has equal representation\n",
    "flagsD = val_split_t['sex_Male']==1#'Black'\n",
    "flagsL = val_split_t['sex_Female']==1#'White'\n",
    "data_L = val_split_t[flagsL]\n",
    "data_L = equalize_dataset_gender(data_L,0.5)\n",
    "\n",
    "data_D = val_split_t[flagsD]\n",
    "data_D = equalize_dataset_gender(data_D,0.5)\n",
    "\n",
    "val_split = data_L.sample(frac=1)\n",
    "\n",
    "\n",
    "test_split_D,test_split_L = resample_dataset_equal(test_labels)\n",
    "\n",
    "print(val_split['salary'].value_counts())\n",
    "print(val_split['sex_Male'].value_counts())\n",
    "print(val_split['sex_Female'].value_counts())\n",
    "\n",
    "val_split = val_split.drop('sex_Male',axis=1)\n",
    "val_split = val_split.drop('sex_Female',axis=1)\n",
    "\n",
    "print(test_split_D['salary'].value_counts())\n",
    "print(test_split_D['sex_Male'].value_counts())\n",
    "print(test_split_D['sex_Female'].value_counts())\n",
    "\n",
    "test_split_D = test_split_D.drop('sex_Male',axis=1)\n",
    "test_split_D = test_split_D.drop('sex_Female',axis=1)\n",
    "\n",
    "print(test_split_L['salary'].value_counts())\n",
    "print(test_split_L['sex_Male'].value_counts())\n",
    "print(test_split_L['sex_Female'].value_counts())\n",
    "\n",
    "test_split_L = test_split_L.drop('sex_Male',axis=1)\n",
    "test_split_L = test_split_L.drop('sex_Female',axis=1)\n",
    "\n",
    "\n",
    "# print(train_split_t.head())\n",
    "print(val_split.shape)\n",
    "print(test_split_D.shape)\n",
    "print(test_split_L.shape)\n",
    "\n",
    "#get the inputs and targets\n",
    "val_x_data = val_split.drop('salary',axis=1)\n",
    "val_y_labels = val_split['salary']\n",
    "\n",
    "test_split_D_x_data = test_split_D.drop('salary',axis=1)\n",
    "test_split_D_y_labels = test_split_D['salary']\n",
    "\n",
    "test_split_L_x_data = test_split_L.drop('salary',axis=1)\n",
    "test_split_L_y_labels = test_split_L['salary']\n",
    "\n",
    "#dataloaders\n",
    "valid_data = AdultsDataset(val_x_data,val_y_labels)\n",
    "test_data_D = AdultsDataset(test_split_D_x_data,test_split_D_y_labels)\n",
    "test_data_L = AdultsDataset(test_split_L_x_data,test_split_L_y_labels)\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "test_loader_D = DataLoader(dataset = test_data_D, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "test_loader_L = DataLoader(dataset = test_data_L, batch_size = batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdkObaX_U7MA"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "def act(x):\n",
    "    return F.relu(x)\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(101, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 50)\n",
    "        self.fcLast = nn.Linear(50,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = act(self.fc1(x))\n",
    "        x = act(self.fc2(x))\n",
    "        x = act(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fcLast(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1636275666157,
     "user": {
      "displayName": "Pradyumna Chari",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18236218636442071478"
     },
     "user_tz": 480
    },
    "id": "QRjrTbWlHWbJ",
    "outputId": "7b0f0f44-b453-4e22-e365-be8812f7d6ef"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmv_MA3mHWbK"
   },
   "outputs": [],
   "source": [
    "def test_performance(model,dataL,criterion):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    temp_test_acc = []\n",
    "\n",
    "    for data, target in dataL:\n",
    "\n",
    "        data = data.to(device).float()\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        # update-average-validation-loss \n",
    "        test_loss += loss.item() * data.size(0)\n",
    "\n",
    "        op_temp = output.detach().cpu().numpy()\n",
    "        op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "        test_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "\n",
    "    ttacc  = test_acc/len(dataL.sampler)\n",
    "    test_loss_M = test_loss/len(dataL.sampler)\n",
    "    \n",
    "    test_print = 'Test Loss: {:.3f} \\tTest Acc: {:.3f}'.format(\n",
    "        test_loss_M, ttacc)\n",
    "\n",
    "    print(test_print)\n",
    "    return test_print, ttacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiUmohixHWbK"
   },
   "outputs": [],
   "source": [
    "def write_file(fname,string,act):\n",
    "    with open(fname, act) as text_file:\n",
    "        text_file.write(string+'\\n')\n",
    "\n",
    "###Combined iteration\n",
    "def train_model(dark_frac,train_split_t,valid_loader,test_loader_D,test_loader_L):\n",
    "\n",
    "    num_epochs = 250\n",
    "    num_classes = 2  # for age\n",
    "#     batch_size = 25\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    check_point_dir = 'male'+str(dark_frac)\n",
    "    \n",
    "    if not os.path.isdir(f\"checkpoints/\"+check_point_dir):\n",
    "        os.makedirs(f\"checkpoints/\"+check_point_dir)\n",
    "        print(\"Output directory is created\")\n",
    "        \n",
    "    #make logger text file\n",
    "    text_path = f\"checkpoints/\"+check_point_dir+\"/\"+\"log.txt\"\n",
    "    try:\n",
    "        os.remove(text_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    write_file(text_path,'********* Male fraction: {} *********'.format(dark_frac),'a')\n",
    "\n",
    "    train_split = resample_dataset_race(train_split_t,dark_frac)\n",
    "\n",
    "    \n",
    "    write_file(text_path,str(train_split['sex_Male'].value_counts()),'a')\n",
    "    write_file(text_path,str(train_split['sex_Female'].value_counts()),'a')\n",
    "    \n",
    "    write_file(text_path,str(train_split['salary'].value_counts()),'a')\n",
    "    \n",
    "    train_split = train_split.drop('sex_Male',axis=1)\n",
    "    train_split = train_split.drop('sex_Female',axis=1)\n",
    "\n",
    "    train_split_x_data = train_split.drop('salary',axis=1)\n",
    "    train_split_y_labels = train_split['salary']\n",
    "\n",
    "\n",
    "#     print(len(train_split))\n",
    "\n",
    "    \n",
    "    #Dataloaders\n",
    "    train_data = AdultsDataset(train_split_x_data,train_split_y_labels)\n",
    "\n",
    "    train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model = Network()\n",
    "    model.load_state_dict(torch.load(f\"model2_init_2class.pt\"))\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        betas=(0.5, 0.999), \n",
    "        weight_decay=0.08\n",
    "        )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=30, \n",
    "        eta_min=0.01 * learning_rate, verbose=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Actual training of model\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    valid_accuracy = []\n",
    "    test_accuracy_D = []\n",
    "    test_accuracy_L = []\n",
    "\n",
    "\n",
    "    print(\"Training model...\")\n",
    "\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # keep track of train/val loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # training the model\n",
    "        model.train()\n",
    "        temp_train_acc = 0.0\n",
    "        for data, target in train_loader:\n",
    "\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()                   # init gradients to zeros\n",
    "            output = model(data)        \n",
    "            loss = criterion(output, target)        # compute loss\n",
    "            loss.backward()                         # loss backwards\n",
    "            optimizer.step()                        # update model params\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            op_temp = output.detach().cpu().numpy()\n",
    "            op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "            temp_train_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "\n",
    "        \n",
    "        # validate-the-model\n",
    "        model.eval()\n",
    "        temp_val_acc = 0.0\n",
    "        for data, target in valid_loader:\n",
    "\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # update-average-validation-loss \n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            op_temp = output.detach().cpu().numpy()\n",
    "            op_temp = np.argmax(op_temp,axis=1)\n",
    "\n",
    "            temp_val_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
    "\n",
    "        tvacc  = np.mean(np.array(temp_val_acc))\n",
    "\n",
    "        if tvacc>best_val_acc:\n",
    "            best_val_acc = tvacc\n",
    "            torch.save(model.state_dict(), f\"checkpoints/\"+check_point_dir+\"/model_best.pt\")\n",
    "            print('Model saved')\n",
    "            write_file(text_path,'Model saved','a')\n",
    "\n",
    "        # calculate-average-losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "        ttacc  = temp_train_acc/len(train_loader.sampler)\n",
    "        tvacc  = temp_val_acc/len(valid_loader.sampler)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        train_accuracies.append(ttacc)\n",
    "        val_accuracies.append(tvacc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # print-training/validation-statistics \n",
    "        train_print = 'Epoch: {} \\tTr Loss: {:.3f} \\tTr Acc: {:.3f} \\tVal Loss: {:.3f} \\tVal Acc: {:.3f}'.format(\n",
    "            epoch, train_loss, ttacc, valid_loss, tvacc)\n",
    "        print(train_print)\n",
    "\n",
    "        test_print_D, ttacc_D = test_performance(model,test_loader_D,criterion)\n",
    "        test_print_L, ttacc_L = test_performance(model,test_loader_L,criterion)\n",
    "        \n",
    "        valid_accuracy.append(tvacc)\n",
    "        test_accuracy_D.append(ttacc_D)\n",
    "        test_accuracy_L.append(ttacc_L)\n",
    "\n",
    "        write_file(text_path,train_print,'a')\n",
    "        \n",
    "        write_file(text_path,test_print_D,'a')\n",
    "        \n",
    "        write_file(text_path,test_print_L,'a')\n",
    "            \n",
    "    path_val = f\"checkpoints/\"+check_point_dir+\"/\"+\"validation_accuracy\"\n",
    "    path_D = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_M\"\n",
    "    path_L = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_F\"\n",
    "    valid_accuracy = np.array(valid_accuracy)\n",
    "    test_accuracy_D = np.array(test_accuracy_D)\n",
    "    test_accuracy_L = np.array(test_accuracy_L)\n",
    "    np.save(path_val, valid_accuracy)\n",
    "    np.save(path_D, test_accuracy_D)\n",
    "    np.save(path_L, test_accuracy_L)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658408,
     "status": "ok",
     "timestamp": 1636276324856,
     "user": {
      "displayName": "Pradyumna Chari",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18236218636442071478"
     },
     "user_tz": 480
    },
    "id": "q6jdcZE4HWbR",
    "outputId": "3f96dc3e-34d5-44f1-b096-bba3102fbc9c"
   },
   "outputs": [],
   "source": [
    "dark_fracs = np.linspace(0.0,1.0,11)\n",
    "for dark_frac in dark_fracs:\n",
    "    print('********* Male fraction: {} *********'.format(dark_frac))\n",
    "    train_model(dark_frac,train_split_t,valid_loader,test_loader_D,test_loader_L)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Adults-IncomeClass-GenderClasses.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
